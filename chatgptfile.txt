Implemente melhorias e correções no projeto, centralizando todas as configurações e caminhos no arquivo configs.yml para facilitar a manutenção e o controle das variáveis. No script file_validation.py, desenvolva as lógicas necessárias para validar arquivos e verifique a existência das pastas envolvidas. Caso alguma pasta não exista, implemente a criação automática.

Antes de renomear os arquivos, certifique-se de que não haja outro arquivo com o mesmo nome, evitando sobreposição. Garanta também que um mesmo arquivo não seja renomeado várias vezes em execuções subsequentes do script, prevenindo inconsistências. Após a validação, transfira os arquivos para a pasta data/raw_validation e, em seguida, exclua-os da pasta raw para evitar duplicidade.

Adicione uma lógica para converter automaticamente arquivos .txt em .csv com base na identificação de padrões no conteúdo. A lógica deve reconhecer colunas e linhas, garantindo que o formato final esteja correto para futuros processamentos e análises.


# main.py
from google.cloud import bigquery
from google.oauth2 import service_account
from config.gcp_secrets import get_service_account_key
from src.data_ingestion import DataIngestion
from src.data_validation import DataValidation
from src.loaders.bigquery_loader import BigQueryLoader

project_number = "1089961646630"
secret_name = "dbt-service-account-key"
project_id = "etl-hacker-quest-ipnet"
dataset_id = "etl_hackerquest"
credentials_path = "credentials/key.json"

# Obter credenciais
credentials_info = get_service_account_key(project_number, secret_name)

# Inicializar DataIngestion
data_ingestion = DataIngestion()
dataframes = data_ingestion.read_data('data/raw/')

# Validar dados
data_validation = DataValidation(dataframes)
data_validation.validate_data()

def create_dataset_if_not_exists(client, dataset_id, project_id):
    """Cria o dataset no BigQuery caso ele não exista."""
    dataset_ref = f"{project_id}.{dataset_id}"
    try:
        client.get_dataset(dataset_ref)  # Verifica se o dataset existe
        print(f"Dataset '{dataset_id}' já existe no projeto '{project_id}'.")
    except Exception as e:
        print(f"Dataset '{dataset_id}' não encontrado. Criando...")
        dataset = bigquery.Dataset(dataset_ref)
        dataset.location = "US"  # Defina a localização conforme necessário
        client.create_dataset(dataset)
        print(f"Dataset '{dataset_id}' criado com sucesso.")

# Inicializar BigQueryLoader
bq_loader = BigQueryLoader(credentials_info, project_id)

# Criar o dataset caso não exista
create_dataset_if_not_exists(bq_loader.client, dataset_id, project_id)

# Carregar dataframes no BigQuery
for file_name, df in dataframes.items():
    table_id = file_name.split('.')[0]  # Nome da tabela é o nome do arquivo sem extensão
    print(f"Carregando {len(df)} linhas na tabela '{table_id}'...")
    bq_loader.load_dataframe(df, dataset_id, table_id)
    print(f"Tabela '{table_id}' carregada com sucesso.")


# Dockerfile
FROM python:3.9-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["python", "main.py"]

# src/file_validation.py

import os
import glob
import shutil
import pandas as pd

from config import *

class FileValidation:
    type_files = ['*.csv', '*.tsv', '*.txt', '*.xlsx', 'xls']

    count_names = {}

    for file in type_files:
        file_name = os.path.basename(file)
        list_files = glob.glob(os.path.join(data_raw_directory, file))

        file_size = os.path.getsize(file)

        if file_size == 0:
                print(f'The file {file} is empty.')
                continue
        
        if file_name in count_names:
            count_names[file_name] += 1
            new_name = f'{os.path.splitext(file_name)[0]}_{count_names[file_name]}{os.path.splitext(file_name)[1]}'

            new_path = os.path.join(data_raw_directory, new_name)
            shutil.move(data_raw_directory, )
            print(f'The file{file_name} was renamed to {new_name}.')
        else:
            count_names[file_name] = 1

# src/data_validation.py
import pandas as pd
from typing import Dict

class DataValidation:
    def __init__(self, dataframes: Dict[str, pd.DataFrame]):
        self.dataframes = dataframes

    def validate_data(self):
        for file_name, df in self.dataframes.items():
            try:
                # Exemplo de validações simples
                if df.empty:
                    raise ValueError(f"O DataFrame do arquivo '{file_name}' está vazio.")
                if df.isnull().values.any():
                    print(f"Aviso: O DataFrame do arquivo '{file_name}' contém valores nulos.")
                else:
                    print(f"O DataFrame do arquivo '{file_name}' passou nas validações.")
            except Exception as e:
                print(f"Erro na validação do arquivo '{file_name}': {e}")

if __name__ == "__main__":
    from data_ingestion import DataIngestion

    data_ingestion = DataInestion()
    dataframes = data_ingestion.read_data('data/raw/')

    data_validation = DataValidation(dataframes)
    data_validation.validate_data()

# src/data_ingestion.py
import pandas as pd
import os
from typing import Dict
from abc import ABC, abstractmethod

class Reader(ABC):
    @abstractmethod
    def read(self, file_path: str) -> pd.DataFrame:
        pass

class CSVReader(Reader):
    def read(self, file_path: str) -> pd.DataFrame:
        return pd.read_csv(file_path)

class TSVReader(Reader):
    def read(self, file_path: str) -> pd.DataFrame:
        return pd.read_csv(file_path, sep='\t')

class ExcelReader(Reader):
    def read(self, file_path: str) -> pd.DataFrame:
        return pd.read_excel(file_path, header=0)
    
class ReaderFactory:
    _readers = {
        '.csv': CSVReader(),
        '.tsv': TSVReader(),
        '.txt': CSVReader(),
        '.xlsx': ExcelReader(),
        '.xls': ExcelReader()
    }

    @classmethod
    def get_reader(cls, extension: str) -> Reader:
        reader = cls._readers.get(extension)
        if reader is None:
            raise ValueError(f"Nenhum leitor encontrado para a extensão '{extension}'")
        return reader

class DataIngestion:
    def read_data(self, path: str) -> Dict[str, pd.DataFrame]:
        dataframes = {}
        files_in_directory = os.listdir(path)

        for file_name in files_in_directory:
            file_path = os.path.join(path, file_name)
            if os.path.isfile(file_path):
                extension = os.path.splitext(file_name)[1].lower()
                try:
                    reader = ReaderFactory.get_reader(extension)
                    df = reader.read(file_path)

                    # Garantir que os nomes das colunas sejam strings
                    df.columns = df.columns.map(str)

                    dataframes[file_name] = df
                    print(f"O arquivo '{file_name}' foi carregado com sucesso.")
                except Exception as e:
                    print(f"Erro ao carregar o arquivo '{file_name}': {e}")
        return dataframes

if __name__ == "__main__":
    data_ingestion = DataIngestion()
    dataframes = data_ingestion.read_data('data/raw/')

# src/loaders/bigquery_loader.py
from google.cloud import bigquery
from google.oauth2 import service_account
import pandas as pd

class BigQueryLoader:
    def __init__(self, credentials_info: dict, project_id: str):
        credentials = service_account.Credentials.from_service_account_info(credentials_info)
        self.client = bigquery.Client(credentials=credentials, project=project_id)

    def validate_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """Valida e ajusta tipos de dados e nomes de colunas para compatibilidade com BigQuery."""
        # Garantir que os nomes das colunas sejam strings
        df.columns = df.columns.map(str)

        # Remover espaços em branco nos nomes das colunas
        df.columns = df.columns.str.strip()

        # Substituir espaços por underscores nos nomes das colunas
        df.columns = df.columns.str.replace(' ', '_')

        # Verificar tipos de dados
        for column in df.columns:
            if pd.api.types.is_object_dtype(df[column]):
                df[column] = df[column].astype(str)
            elif pd.api.types.is_integer_dtype(df[column]):
                df[column] = df[column].astype('Int64')  # Tipo compatível com BigQuery
            elif pd.api.types.is_float_dtype(df[column]):
                df[column] = df[column].astype(float)
            elif pd.api.types.is_bool_dtype(df[column]):
                df[column] = df[column].astype(bool)
            else:
                # Converte outros tipos para string por segurança
                df[column] = df[column].astype(str)
        return df

    def load_dataframe(self, df: pd.DataFrame, dataset_id: str, table_id: str):
        """Carrega o DataFrame no BigQuery."""
        df = self.validate_dataframe(df)  # Valida e ajusta tipos de dados

        table_ref = f"{self.client.project}.{dataset_id}.{table_id}"
        print(f"Iniciando carga para a tabela '{table_ref}'...")

        job_config = bigquery.LoadJobConfig(
            write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE
        )

        job = self.client.load_table_from_dataframe(df, table_ref, job_config=job_config)
        job.result()  # Aguarda o término do job
        print(f"Carregado {len(df)} linhas na tabela '{table_ref}'.")

#dbt_validations/.user.yml
id: f7654f3a-33ee-4e83-b165-2c413c7aa682

# dbt_validations/dbt_project.yml
name: etl_hackerquest
version: '1.0'
profile: default
config-version: 2

models:
  etl_hackerquest:
    staging:
      materialized: view
    marts:
      materialized: table

# dbt_validations/profiles.yml
default:
  target: dev
  outputs:
    dev:
      type: bigquery
      method: service-account-json
      project: etl-hacker-quest-ipnet
      dataset: etl_hackerquest
      threads: 4
      keyfile_json: '{{ env_var("DBT_KEYFILE_JSON") }}'

# dbt_validations/models/stg_hackers.sql
-- dbt/models/staging/stg_hackers.sql
select * from {{ source('etl_hackerquest', 'hackers') }}

# data/raw
    challenges.txt
    diffculty.tsv
    diffculty.xlsx
    hackers.csv
    submissions.tsv

# data/raw_validations/

# credentials/key.json

# config/gcp_secrets.py
import json
from google.cloud import secretmanager

def get_service_account_key(project_number, secret_name, version_id='latest'):
    client = secretmanager.SecretManagerServiceClient()
    name = f"projects/{project_number}/secrets/{secret_name}/versions/{version_id}"
    response = client.access_secret_version(request={"name": name})
    return json.loads(response.payload.data.decode("UTF-8"))

# config/configs.yml
data_raw_directory = '../data/raw'
data_validation_directory = '../data/raw_validated'